import time
import os
import collections
import queue
import wave
import struct
import numpy as np
import sounddevice as sd
import webrtcvad
from typing import List, Optional, Generator

class ChatAI:
    """
    è´Ÿè´£å¤„ç†å”¤é†’åçš„å¯¹è¯é€»è¾‘
    é›†æˆ VAD (Voice Activity Detection) å®ç°è‡ªåŠ¨å¬å†™åˆ‡åˆ†
    """
    def __init__(self):
        # é€€å‡ºå¯¹è¯çš„å…³é”®è¯åˆ—è¡¨
        self.exit_keywords = ["è°¢è°¢", "å†è§", "ç»“æŸ", "é€€ä¸‹", "exit", "quit", "bye"]
        # å¯¹è¯è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        self.timeout_seconds = 20 # å¢åŠ è¶…æ—¶æ—¶é—´ï¼Œé€‚åº”è¯­éŸ³äº¤äº’
        # è¿ç»­æ— æ•ˆè¾“å…¥çš„å…è®¸æ¬¡æ•°
        self.max_invalid_inputs = 3
        
        # Audio / VAD é…ç½®
        self.sample_rate = 16000
        self.frame_duration_ms = 30  # 10, 20, or 30ms
        self.vad_aggressiveness = 3  # 0-3, 3 is most aggressive in filtering non-speech
        self.vad = webrtcvad.Vad(self.vad_aggressiveness)
        
        # VAD ç®—æ³•å‚æ•°
        self.padding_duration_ms = 300  # è¯­éŸ³å¼€å§‹/ç»“æŸå‰åçš„ç¼“å†²æ—¶é•¿
        self.frame_prop_duration_ms = self.frame_duration_ms 
        
        # å½•éŸ³å‚æ•°
        self.channels = 1
        self.dtype = 'int16'
        self.block_size = int(self.sample_rate * self.frame_duration_ms / 1000)
        
    def start_dialogue(self) -> List[dict]:
        """
        å¼€å§‹å¯¹è¯æµç¨‹
        """
        print("\n" + "="*30)
        print("ğŸ¤– Jarvis: I'm listening... (Speak to microphone)")
        print("="*30 + "\n")

        conversation_history = []
        last_input_time = time.time()
        invalid_input_count = 0

        while True:
            try:
                # 1. æ£€æŸ¥è¶…æ—¶
                if time.time() - last_input_time > self.timeout_seconds:
                    print(f"\n[System] Timeout: No interaction for {self.timeout_seconds}s. Going back to sleep.")
                    break

                # 2. ç›‘å¬å¹¶è·å–æ–‡æœ¬ (VAD -> Speech -> ASR -> Text)
                text_input = self._listen_and_transcribe()

                # 3. å¤„ç†æ— æ•ˆè¾“å…¥ (æœªæ£€æµ‹åˆ°è¯­éŸ³ æˆ– ASRä¸ºç©º)
                if not text_input:
                    # å¦‚æœåªæ˜¯æ²¡å¬æ¸…ï¼Œæš‚æ—¶ä¸è®¡å…¥ä¸¥æ ¼çš„æ— æ•ˆæ¬¡æ•°ï¼Œæˆ–è€…å¯ä»¥å®½æ¾å¤„ç†
                    # è¿™é‡Œç®€å•çš„é€»è¾‘ï¼šå¦‚æœè¿ç»­å¤šæ¬¡å•¥éƒ½æ²¡å¬åˆ°ï¼Œå¯èƒ½ç”¨æˆ·èµ°äº†
                    invalid_input_count += 1
                    if invalid_input_count >= self.max_invalid_inputs:
                        print("\n[System] Too many failed attempts. Ending conversation.")
                        break
                    continue
                
                # æœ‰æ•ˆè¾“å…¥ï¼Œé‡ç½®è®¡æ•°å™¨
                last_input_time = time.time()
                invalid_input_count = 0
                
                print(f"User: {text_input}")

                # 4. æ£€æŸ¥æ˜¯å¦åŒ…å«ç»“æŸè¯
                if self._check_exit_intent(text_input):
                    print("ğŸ¤– Jarvis: Goodbye!")
                    break

                # 5. ç”Ÿæˆå›å¤ (Mock LLM)
                ai_response = self._process_response(text_input)
                print(f"ğŸ¤– Jarvis: {ai_response}")

                # è®°å½•å¯¹è¯
                conversation_history.append({"role": "user", "content": text_input})
                conversation_history.append({"role": "ai", "content": ai_response})
                
                # TODO: è¿™é‡Œæ·»åŠ  TTS (Text-to-Speech) æ’­æ”¾å›å¤
                # self._play_audio(ai_response)

            except KeyboardInterrupt:
                print("\n[System] Interrupted by user.")
                break

        print("\n" + "="*30)
        print("ğŸ˜´ Jarvis: Entering sleep mode...")
        print("="*30 + "\n")
        
        return conversation_history

    def _listen_and_transcribe(self) -> str:
        """
        æ ¸å¿ƒæµç¨‹ï¼šç›‘å¬éº¦å…‹é£ -> VADåˆ‡åˆ†è¯­éŸ³ -> ASRè¯†åˆ« -> è¿”å›æ–‡æœ¬
        """
        print(">> Listening...", end="", flush=True)
        
        # 1. å½•åˆ¶è¯­éŸ³ç‰‡æ®µ (é˜»å¡ç›´åˆ°è¯´è¯ç»“æŸ)
        audio_data = self._record_speech_segment()
        
        if not audio_data:
            print(" [Silence detected]")
            return ""
            
        print(f" [Captured {len(audio_data)} bytes audio]")
        
        # 2. è¯­éŸ³è½¬æ–‡å­— (ASR)
        text = self._asr_engine(audio_data)
        return text

    def _record_speech_segment(self) -> bytes:
        """
        ä½¿ç”¨ VAD å½•åˆ¶ä¸€æ®µæœ‰æ•ˆçš„è¯­éŸ³
        é€»è¾‘ï¼š
        - æŒç»­è¯»å–éŸ³é¢‘æµ
        - ç»´æŠ¤ä¸€ä¸ªç¯å½¢ç¼“å†²åŒº (RingBuffer) å­˜å‚¨æœ€è¿‘çš„éŸ³é¢‘å¸§
        - å½“æ£€æµ‹åˆ°è§¦å‘çŠ¶æ€ (Triggered) æ—¶ï¼Œå¼€å§‹å½•åˆ¶
        - å½“è¿ç»­é™éŸ³è¶…è¿‡ä¸€å®šæ—¶é•¿ï¼Œåœæ­¢å½•åˆ¶
        """
        num_padding_frames = int(self.padding_duration_ms / self.frame_duration_ms)
        ring_buffer = collections.deque(maxlen=num_padding_frames)
        
        triggered = False
        voiced_frames = []
        
        # æ²‰é»˜å¸§è®¡æ•°å™¨ï¼Œç”¨äºåˆ¤æ–­è¯­éŸ³ç»“æŸ
        silent_frame_count = 0
        max_silent_frames = int(800 / self.frame_duration_ms) # åœæ­¢å‰çš„æœ€å¤§é™éŸ³æ—¶é•¿ (ä¾‹å¦‚ 800ms)

        # ä½¿ç”¨ RawInputStream è¯»å–åŸå§‹å­—èŠ‚æµ
        with sd.RawInputStream(
            samplerate=self.sample_rate, 
            blocksize=self.block_size, 
            dtype=self.dtype, 
            channels=self.channels
        ) as stream:
            
            # æœ€å¤§å½•éŸ³æ—¶é•¿ä¿æŠ¤ (ä¾‹å¦‚ 15ç§’)
            max_frames = int(15000 / self.frame_duration_ms)
            frame_count = 0
            
            while True:
                # è¯»å–éŸ³é¢‘å—
                data, overflow = stream.read(self.block_size)
                if overflow:
                    pass # å¿½ç•¥æº¢å‡ºè­¦å‘Š

                # VAD æ£€æµ‹
                is_speech = self.vad.is_speech(data, self.sample_rate)

                if not triggered:
                    ring_buffer.append((data, is_speech))
                    
                    # è§¦å‘é€»è¾‘ï¼šå¦‚æœç¯å½¢ç¼“å†²åŒºä¸­è¶…è¿‡ 90% çš„å¸§æ˜¯è¯­éŸ³ï¼Œåˆ™è§¦å‘å¼€å§‹
                    num_voiced = len([f for f, speech in ring_buffer if speech])
                    if num_voiced > 0.9 * ring_buffer.maxlen:
                        triggered = True
                        print("\n[Speech Detected] Recording...", end="", flush=True)
                        # å°†ç¼“å†²åŒºçš„å†…å®¹åŠ å…¥å½•åˆ¶åˆ—è¡¨
                        for f, s in ring_buffer:
                            voiced_frames.append(f)
                        ring_buffer.clear()
                else:
                    # å·²è§¦å‘ï¼Œæ­£åœ¨å½•åˆ¶
                    voiced_frames.append(data)
                    frame_count += 1
                    
                    if is_speech:
                        silent_frame_count = 0 
                    else:
                        silent_frame_count += 1
                    
                    # ç»“æŸé€»è¾‘ 1: è¿ç»­é™éŸ³è¶³å¤Ÿé•¿
                    if silent_frame_count > max_silent_frames:
                        print(" [End of speech]")
                        break
                    
                    # ç»“æŸé€»è¾‘ 2: è¾¾åˆ°æœ€å¤§æ—¶é•¿
                    if frame_count > max_frames:
                        print(" [Max duration reached]")
                        break
        
        # å¦‚æœå½•åˆ¶çš„å¸§æ•°å¤ªå°‘ï¼ˆä¾‹å¦‚åªæ˜¯ä¸€ä¸ªå™ªéŸ³ï¼‰ï¼Œåˆ™å¿½ç•¥
        if len(voiced_frames) < 10:
            return b""
            
        return b''.join(voiced_frames)

    def _asr_engine(self, audio_data: bytes) -> str:
        """
        Mock ASR å¼•æ“
        TODO: åœ¨è¿™é‡Œé›†æˆå®é™…çš„ ASR æ¨¡å‹ (å¦‚ OpenAI Whisper, Google Speech Recognition ç­‰)
        """
        # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬å¯ä»¥æŠŠéŸ³é¢‘ä¿å­˜ä¸‹æ¥ï¼Œæ–¹ä¾¿è°ƒè¯•
        self._save_wav(audio_data, "last_speech.wav")
        
        # è¿”å›æ¨¡æ‹Ÿæ–‡æœ¬
        # å®é™…é¡¹ç›®ä¸­ï¼Œåœ¨è¿™é‡Œè°ƒç”¨ï¼š return whisper_model.transcribe("last_speech.wav")['text']
        print("\n[ASR] (Simulating recognition...)")
        
        # æš‚æ—¶è¿”å›å›ºå®šæ–‡æœ¬ç”¨äºæµ‹è¯•å¤šè½®å¯¹è¯
        # ä½ å¯ä»¥åœ¨è¿™é‡Œè®©å®ƒç¨å¾®éšæœºä¸€ç‚¹ï¼Œæˆ–è€…æ ¹æ®å½•éŸ³é•¿åº¦å˜åŒ–
        if len(audio_data) < 32000: # å¾ˆçŸ­çš„å£°éŸ³
            return ""
            
        return "ä½ å¥½ Jarvisï¼Œè¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å¯¹è¯ã€‚"

    def _save_wav(self, audio_data: bytes, filename: str):
        """ä¿å­˜éŸ³é¢‘æ•°æ®åˆ° wav æ–‡ä»¶"""
        path = os.path.join(os.path.dirname(__file__), "recordings", filename)
        os.makedirs(os.path.dirname(path), exist_ok=True)
        with wave.open(path, 'wb') as wf:
            wf.setnchannels(self.channels)
            wf.setsampwidth(2) # 16-bit = 2 bytes
            wf.setframerate(self.sample_rate)
            wf.writeframes(audio_data)
        # print(f"[Debug] Audio saved to {path}")

    def _check_exit_intent(self, text: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰é€€å‡ºæ„å›¾"""
        text_lower = text.lower()
        for kw in self.exit_keywords:
            if kw in text_lower:
                return True
        return False

    def _process_response(self, text: str) -> str:
        """
        å¤„ç†ç”¨æˆ·è¾“å…¥å¹¶ç”Ÿæˆå›å¤
        """
        if "ä½ å¥½" in text:
            return "ä½ å¥½ï¼å¾ˆé«˜å…´ä¸ºä½ æœåŠ¡ã€‚"
        if "å‡ ç‚¹" in text or "æ—¶é—´" in text:
            return f"ç°åœ¨æ˜¯ {time.strftime('%H:%M')}ã€‚"
        return f"æˆ‘å¬åˆ°äº†ï¼š{text}ï¼Œä½†æˆ‘è¿˜ä¸çŸ¥é“æ€ä¹ˆå›ç­”ã€‚"
